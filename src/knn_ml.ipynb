{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35861ff3-1c45-4fd3-92f9-9e997c86741a",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors Model Development\n",
    "\n",
    "The purpose of this notebook is to:\n",
    "\n",
    "1. Read in the preprocessed (feature engineered) dataset\n",
    "2. Declare a parameter grid\n",
    "3. Perform the machine learning cross validation pipeline\n",
    "4. Train and export a final model and test scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d06541-8e0b-41ad-8d92-336a6aab995b",
   "metadata": {},
   "source": [
    "## Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a92aa3d-097c-4920-8929-7b0e5e6312cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0af726-498e-42e5-9828-22cb95f2eff3",
   "metadata": {},
   "source": [
    "## Read in the preprocessed (feature engineered) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0fde699-3b97-4ac2-962f-4de6299ab402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['Name', 'Grade', 'Section', 'Class', 'School', 'Race', 'Date', 'Place',\n",
       "        'Time (sec)', 'Speed Rating', 'SR', 'Gender', 'Race Section',\n",
       "        'Latitude', 'Longitude', 'Temperature', 'Cloud Coverage', 'Wind Speed',\n",
       "        'Precipitation', 'Dew Point', 'Humidity', 'Wind Chill', 'Wind Gust',\n",
       "        'Heat Index', 'Visibility', 'Distance (mi)', 'Time-Place',\n",
       "        'Name-School', 'Speed (mi/sec)', 'Average_Time',\n",
       "        'Time_Difference_From_Avg', 'Average_Time_Class',\n",
       "        'Time_Difference_From_Avg_Class', 'First_Place_Time',\n",
       "        'Time_Difference_First_Place', 'First_Place_Time_Class',\n",
       "        'Time_Difference_First_Place_Class', 'Average_Speed',\n",
       "        'Speed_Difference_From_Avg', 'Temp_Humidity', 'WindSpeed_WindChill',\n",
       "        'Temp_WindSpeed', 'Humidity_WindSpeed', 'HeatIndex_Humidity',\n",
       "        'DewPoint_Temperature', 'DewPoint_Humidity', 'DewPoint_WindSpeed',\n",
       "        'Year', 'Month', 'Day'],\n",
       "       dtype='object'),\n",
       " (330108, 50))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/preprocessed.csv\")\n",
    "df.columns, df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5288c1b5-e33b-47cf-944a-81b31fd6831e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"Speed Rating\", \"SR\"])\n",
    "y = df[\"Speed Rating\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6c890ad-0667-4281-8e96-fa027816a5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_ftrs = ['Date', 'Name', 'Name-School', 'Race Section', 'School', 'Race']\n",
    "cat_ftrs = ['Gender', 'Section']\n",
    "ordinal_ftrs = ['Class', 'Grade', 'Year', 'Month', 'Day']\n",
    "ordinal_cats = [['D','C','B','CITY','A', 'AA'],[7,8,9,10,11,12,13], [2014,2015,2016,2017,2018,2019], [1,2,3,4,5,6,7,8,9,10,11,12], \\\n",
    "                [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31]]\n",
    "num_ftrs = ['Place', 'Time (sec)', 'Temperature', 'Cloud Coverage', 'Wind Speed', \\\n",
    "            'Precipitation', 'Dew Point', 'Humidity', 'Visibility', 'Time-Place', 'Speed_Difference_From_Avg', \\\n",
    "            'Distance (mi)', 'Speed (mi/sec)', 'Average_Time', 'Average_Speed', 'Time_Difference_From_Avg', 'Average_Time_Class', \\\n",
    "            'Time_Difference_From_Avg_Class', 'Temp_Humidity', 'WindSpeed_WindChill', 'Temp_WindSpeed', 'Humidity_WindSpeed', \\\n",
    "            'HeatIndex_Humidity', 'DewPoint_Temperature', 'DewPoint_Humidity', 'DewPoint_WindSpeed', 'Latitude', 'Longitude', \\\n",
    "            'Time_Difference_First_Place', 'Time_Difference_First_Place_Class','First_Place_Time_Class', 'First_Place_Time', \\\n",
    "            'Wind Chill', 'Wind Gust', 'Heat Index']\n",
    "\n",
    "# one-hot encoder\n",
    "# We need to replace the NaN with a string first!\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant',fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(sparse_output=False,handle_unknown='ignore'))])\n",
    "\n",
    "# ordinal encoder\n",
    "# We need to replace the NaN with a string first!\n",
    "ordinal_transformer = Pipeline(steps=[\n",
    "    ('imputer2', SimpleImputer(strategy='constant',fill_value='NA')),\n",
    "    ('ordinal', OrdinalEncoder(categories = ordinal_cats))])\n",
    "\n",
    "# standard scaler\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "# collect the transformers\n",
    "prep = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_ftrs),\n",
    "        ('cat', categorical_transformer, cat_ftrs),\n",
    "        ('ord', ordinal_transformer, ordinal_ftrs)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e4036d-6fd8-42db-9655-80ff0f967570",
   "metadata": {},
   "source": [
    "## Declare a parameter grid\n",
    "\n",
    "*among other variables*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "141cf5d3-0c28-40da-8a73-69854090cb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_states = 5\n",
    "\n",
    "param_grid = {\n",
    "    \"n_neighbors\": [1,3,5,7,9,13,18,21],\n",
    "    \"weights\": [\"uniform\", \"distance\"]\n",
    "}\n",
    "\n",
    "unique_years = sorted(df['Year'].unique().tolist())\n",
    "\n",
    "final_models = []\n",
    "test_scores = []\n",
    "\n",
    "def fill_nan_col(data, model, column='num__Wind Gust'):\n",
    "    data_with = data[data[column].notnull()]\n",
    "    data_missing = data[data[column].isnull()]\n",
    "\n",
    "    X_missing = data_missing.drop(columns=[column])\n",
    "    predicted_values = model.predict(X_missing)\n",
    "\n",
    "    data.loc[data[column].isnull(), column] = predicted_values\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b695eb7-56ad-4d15-bf45-0c204c60b6aa",
   "metadata": {},
   "source": [
    "## Perform the machine learning cross validation pipeline\n",
    "\n",
    "The pipeline is as follows:\n",
    "\n",
    "1. Loop through random states\n",
    "2. Loop through time series iterations (split by year)\n",
    "3. Find best model by training on train sets and saving best val RMSE score\n",
    "4. Save the model and test score for each random state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f147a67-c151-4bca-93cf-ed63d707e1a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RANDOM STATE 1 OF 5\n",
      "  time series split 1: training 2014 to 2014, val 2015, test 2016\n",
      "  split: train (62353, 56) val (64070, 56) test (65411, 56)\n",
      "     {'n_neighbors': 1, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 1, 'weights': 'distance'}\n",
      "     {'n_neighbors': 3, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 3, 'weights': 'distance'}\n",
      "     {'n_neighbors': 5, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 5, 'weights': 'distance'}\n",
      "     {'n_neighbors': 7, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 7, 'weights': 'distance'}\n",
      "     {'n_neighbors': 9, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 9, 'weights': 'distance'}\n",
      "     {'n_neighbors': 13, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 13, 'weights': 'distance'}\n",
      "     {'n_neighbors': 18, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 18, 'weights': 'distance'}\n",
      "     {'n_neighbors': 21, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 21, 'weights': 'distance'}\n",
      "    test score: 16.28251093177203\n",
      "  time series split 2: training 2014 to 2015, val 2016, test 2017\n",
      "  split: train (126423, 56) val (65411, 56) test (64158, 56)\n",
      "     {'n_neighbors': 1, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 1, 'weights': 'distance'}\n",
      "     {'n_neighbors': 3, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 3, 'weights': 'distance'}\n",
      "     {'n_neighbors': 5, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 5, 'weights': 'distance'}\n",
      "     {'n_neighbors': 7, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 7, 'weights': 'distance'}\n",
      "     {'n_neighbors': 9, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 9, 'weights': 'distance'}\n",
      "     {'n_neighbors': 13, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 13, 'weights': 'distance'}\n",
      "     {'n_neighbors': 18, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 18, 'weights': 'distance'}\n",
      "     {'n_neighbors': 21, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 21, 'weights': 'distance'}\n",
      "    test score: 15.755394018985559\n",
      "\n",
      "RANDOM STATE 2 OF 5\n",
      "  time series split 1: training 2014 to 2014, val 2015, test 2016\n",
      "  split: train (62353, 56) val (64070, 56) test (65411, 56)\n",
      "     {'n_neighbors': 1, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 1, 'weights': 'distance'}\n",
      "     {'n_neighbors': 3, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 3, 'weights': 'distance'}\n",
      "     {'n_neighbors': 5, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 5, 'weights': 'distance'}\n",
      "     {'n_neighbors': 7, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 7, 'weights': 'distance'}\n",
      "     {'n_neighbors': 9, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 9, 'weights': 'distance'}\n",
      "     {'n_neighbors': 13, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 13, 'weights': 'distance'}\n",
      "     {'n_neighbors': 18, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 18, 'weights': 'distance'}\n",
      "     {'n_neighbors': 21, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 21, 'weights': 'distance'}\n",
      "    test score: 16.28251093177203\n",
      "  time series split 2: training 2014 to 2015, val 2016, test 2017\n",
      "  split: train (126423, 56) val (65411, 56) test (64158, 56)\n",
      "     {'n_neighbors': 1, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 1, 'weights': 'distance'}\n",
      "     {'n_neighbors': 3, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 3, 'weights': 'distance'}\n",
      "     {'n_neighbors': 5, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 5, 'weights': 'distance'}\n",
      "     {'n_neighbors': 7, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 7, 'weights': 'distance'}\n",
      "     {'n_neighbors': 9, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 9, 'weights': 'distance'}\n",
      "     {'n_neighbors': 13, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 13, 'weights': 'distance'}\n",
      "     {'n_neighbors': 18, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 18, 'weights': 'distance'}\n",
      "     {'n_neighbors': 21, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 21, 'weights': 'distance'}\n",
      "    test score: 15.755394018985559\n",
      "\n",
      "RANDOM STATE 3 OF 5\n",
      "  time series split 1: training 2014 to 2014, val 2015, test 2016\n",
      "  split: train (62353, 56) val (64070, 56) test (65411, 56)\n",
      "     {'n_neighbors': 1, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 1, 'weights': 'distance'}\n",
      "     {'n_neighbors': 3, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 3, 'weights': 'distance'}\n",
      "     {'n_neighbors': 5, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 5, 'weights': 'distance'}\n",
      "     {'n_neighbors': 7, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 7, 'weights': 'distance'}\n",
      "     {'n_neighbors': 9, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 9, 'weights': 'distance'}\n",
      "     {'n_neighbors': 13, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 13, 'weights': 'distance'}\n",
      "     {'n_neighbors': 18, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 18, 'weights': 'distance'}\n",
      "     {'n_neighbors': 21, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 21, 'weights': 'distance'}\n",
      "    test score: 16.28251093177203\n",
      "  time series split 2: training 2014 to 2015, val 2016, test 2017\n",
      "  split: train (126423, 56) val (65411, 56) test (64158, 56)\n",
      "     {'n_neighbors': 1, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 1, 'weights': 'distance'}\n",
      "     {'n_neighbors': 3, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 3, 'weights': 'distance'}\n",
      "     {'n_neighbors': 5, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 5, 'weights': 'distance'}\n",
      "     {'n_neighbors': 7, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 7, 'weights': 'distance'}\n",
      "     {'n_neighbors': 9, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 9, 'weights': 'distance'}\n",
      "     {'n_neighbors': 13, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 13, 'weights': 'distance'}\n",
      "     {'n_neighbors': 18, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 18, 'weights': 'distance'}\n",
      "     {'n_neighbors': 21, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 21, 'weights': 'distance'}\n",
      "    test score: 15.755394018985559\n",
      "\n",
      "RANDOM STATE 4 OF 5\n",
      "  time series split 1: training 2014 to 2014, val 2015, test 2016\n",
      "  split: train (62353, 56) val (64070, 56) test (65411, 56)\n",
      "     {'n_neighbors': 1, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 1, 'weights': 'distance'}\n",
      "     {'n_neighbors': 3, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 3, 'weights': 'distance'}\n",
      "     {'n_neighbors': 5, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 5, 'weights': 'distance'}\n",
      "     {'n_neighbors': 7, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 7, 'weights': 'distance'}\n",
      "     {'n_neighbors': 9, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 9, 'weights': 'distance'}\n",
      "     {'n_neighbors': 13, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 13, 'weights': 'distance'}\n",
      "     {'n_neighbors': 18, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 18, 'weights': 'distance'}\n",
      "     {'n_neighbors': 21, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 21, 'weights': 'distance'}\n",
      "    test score: 16.28251093177203\n",
      "  time series split 2: training 2014 to 2015, val 2016, test 2017\n",
      "  split: train (126423, 56) val (65411, 56) test (64158, 56)\n",
      "     {'n_neighbors': 1, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 1, 'weights': 'distance'}\n",
      "     {'n_neighbors': 3, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 3, 'weights': 'distance'}\n",
      "     {'n_neighbors': 5, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 5, 'weights': 'distance'}\n",
      "     {'n_neighbors': 7, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 7, 'weights': 'distance'}\n",
      "     {'n_neighbors': 9, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 9, 'weights': 'distance'}\n",
      "     {'n_neighbors': 13, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 13, 'weights': 'distance'}\n",
      "     {'n_neighbors': 18, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 18, 'weights': 'distance'}\n",
      "     {'n_neighbors': 21, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 21, 'weights': 'distance'}\n",
      "    test score: 15.755394018985559\n",
      "\n",
      "RANDOM STATE 5 OF 5\n",
      "  time series split 1: training 2014 to 2014, val 2015, test 2016\n",
      "  split: train (62353, 56) val (64070, 56) test (65411, 56)\n",
      "     {'n_neighbors': 1, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 1, 'weights': 'distance'}\n",
      "     {'n_neighbors': 3, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 3, 'weights': 'distance'}\n",
      "     {'n_neighbors': 5, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 5, 'weights': 'distance'}\n",
      "     {'n_neighbors': 7, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 7, 'weights': 'distance'}\n",
      "     {'n_neighbors': 9, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 9, 'weights': 'distance'}\n",
      "     {'n_neighbors': 13, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 13, 'weights': 'distance'}\n",
      "     {'n_neighbors': 18, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 18, 'weights': 'distance'}\n",
      "     {'n_neighbors': 21, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 21, 'weights': 'distance'}\n",
      "    test score: 16.28251093177203\n",
      "  time series split 2: training 2014 to 2015, val 2016, test 2017\n",
      "  split: train (126423, 56) val (65411, 56) test (64158, 56)\n",
      "     {'n_neighbors': 1, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 1, 'weights': 'distance'}\n",
      "     {'n_neighbors': 3, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 3, 'weights': 'distance'}\n",
      "     {'n_neighbors': 5, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 5, 'weights': 'distance'}\n",
      "     {'n_neighbors': 7, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 7, 'weights': 'distance'}\n",
      "     {'n_neighbors': 9, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 9, 'weights': 'distance'}\n",
      "     {'n_neighbors': 13, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 13, 'weights': 'distance'}\n",
      "     {'n_neighbors': 18, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 18, 'weights': 'distance'}\n",
      "     {'n_neighbors': 21, 'weights': 'uniform'}\n",
      "     {'n_neighbors': 21, 'weights': 'distance'}\n",
      "    test score: 15.755394018985559\n"
     ]
    }
   ],
   "source": [
    "for rand_state in range(nr_states):\n",
    "    print('\\nRANDOM STATE', rand_state + 1, 'OF', nr_states)\n",
    "\n",
    "    state_best_model = None\n",
    "    state_best_model_test_score = None\n",
    "    state_best_val_score = float('inf')\n",
    "\n",
    "    for i, year in enumerate(unique_years[:-4]):\n",
    "        val_year, test_year = year + 1, year + 2\n",
    "        print(f\"  time series split {i+1}: training {min(unique_years)} to {year}, val {val_year}, test {test_year}\")\n",
    "\n",
    "        # split\n",
    "        train_years_condition = (X['Year'] >= min(unique_years)) & (X['Year'] <= year)\n",
    "        X_train, y_train = X[train_years_condition], y[train_years_condition]\n",
    "        X_val, y_val = X[(X['Year'] == val_year)], y[(X['Year'] == val_year)]\n",
    "        X_test, y_test = X[(X['Year'] == test_year)], y[(X['Year'] == test_year)]\n",
    "\n",
    "        # fit train, transform train-val-test sets\n",
    "        prep.fit(X_train.drop(columns=drop_ftrs))\n",
    "        feature_names = prep.get_feature_names_out()\n",
    "        train_prep = pd.DataFrame(prep.transform(X_train.drop(columns=drop_ftrs)), columns=feature_names)\n",
    "        val_prep = pd.DataFrame(prep.transform(X_val.drop(columns=drop_ftrs)), columns=feature_names)\n",
    "        test_prep = pd.DataFrame(prep.transform(X_test.drop(columns=drop_ftrs)), columns=feature_names)\n",
    "        print(\"  split: train\", train_prep.shape, \"val\", val_prep.shape, \"test\", test_prep.shape)\n",
    "\n",
    "        # xgb missing values\n",
    "        train_with_gust = train_prep[train_prep['num__Wind Gust'].notnull()]\n",
    "        train_missing_gust = train_prep[train_prep['num__Wind Gust'].isnull()]\n",
    "        \n",
    "        X_train_gust = train_with_gust.drop(columns=['num__Wind Gust'])\n",
    "        y_train_gust = train_with_gust['num__Wind Gust']\n",
    "        \n",
    "        xgb_gust_model = XGBRegressor(random_state=42*(rand_state + 1))\n",
    "        xgb_gust_model.fit(X_train_gust, y_train_gust)\n",
    "        \n",
    "        train_prep = fill_nan_col(train_prep, xgb_gust_model)\n",
    "        val_prep = fill_nan_col(val_prep, xgb_gust_model)\n",
    "        test_prep = fill_nan_col(test_prep, xgb_gust_model)\n",
    "\n",
    "        # Hyperparameter tuning and model selection\n",
    "        iteration_models, train_scores, val_scores = [], [], []\n",
    "\n",
    "        for params in ParameterGrid(param_grid):\n",
    "            print('    ', params)\n",
    "            model = KNeighborsRegressor(**params)\n",
    "            model.fit(train_prep, y_train)\n",
    "            iteration_models.append(model)\n",
    "            \n",
    "            y_val_pred = model.predict(val_prep)\n",
    "            val_score = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "            val_scores.append(val_score)\n",
    "\n",
    "            y_train_pred = model.predict(train_prep)\n",
    "            train_score = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "            train_scores.append(train_score)\n",
    "\n",
    "        # Evaluate the best model on the test set\n",
    "        best_iteration_model = iteration_models[np.argmin(val_scores)]\n",
    "        y_test_pred = best_iteration_model.predict(test_prep)\n",
    "        test_score = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "        print('    test score:', test_score)\n",
    "\n",
    "        # Update the best model for the state if necessary\n",
    "        if min(val_scores) < state_best_val_score:\n",
    "            state_best_val_score = min(val_scores)\n",
    "            state_best_model = best_iteration_model\n",
    "            state_best_model_test_score = test_score\n",
    "\n",
    "    final_models.append(state_best_model)\n",
    "    test_scores.append(state_best_model_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25b5d51c-9662-43b1-ad8f-46257ffe54e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean test score: 16.28251093177203 std: 0.0\n"
     ]
    }
   ],
   "source": [
    "print('mean test score:', np.mean(test_scores), 'std:', np.std(test_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b1bcd1-c7bd-4843-900e-90ea7de32b90",
   "metadata": {},
   "source": [
    "## Train and export a final model and test scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "894d4c02-8983-490d-bccc-cad323d65f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting\n",
      "preprocessing\n",
      "filling missing values\n",
      "training best model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16.524620445235147"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split\n",
    "print(\"splitting\")\n",
    "train_years_condition = (X['Year'] >= 2014) & (X['Year'] <= 2018)\n",
    "X_train, y_train = X[train_years_condition], y[train_years_condition]\n",
    "X_test, y_test = X[(X['Year'] == 2019)], y[(X['Year'] == 2019)]\n",
    "\n",
    "# fit train, transform train-val-test sets\n",
    "print(\"preprocessing\")\n",
    "prep.fit(X_train.drop(columns=drop_ftrs))\n",
    "feature_names = prep.get_feature_names_out()\n",
    "train_prep = pd.DataFrame(prep.transform(X_train.drop(columns=drop_ftrs)), columns=feature_names)\n",
    "test_prep = pd.DataFrame(prep.transform(X_test.drop(columns=drop_ftrs)), columns=feature_names)\n",
    "\n",
    "# xgb missing values\n",
    "print(\"filling missing values\")\n",
    "train_with_gust = train_prep[train_prep['num__Wind Gust'].notnull()]\n",
    "train_missing_gust = train_prep[train_prep['num__Wind Gust'].isnull()]\n",
    "\n",
    "X_train_gust = train_with_gust.drop(columns=['num__Wind Gust'])\n",
    "y_train_gust = train_with_gust['num__Wind Gust']\n",
    "\n",
    "xgb_gust_model = XGBRegressor(random_state=42*rand_state)\n",
    "xgb_gust_model.fit(X_train_gust, y_train_gust)\n",
    "\n",
    "train_prep = fill_nan_col(train_prep, xgb_gust_model)\n",
    "test_prep = fill_nan_col(test_prep, xgb_gust_model)\n",
    "\n",
    "# best model :)\n",
    "print(\"training best model\")\n",
    "best_final_model = final_models[np.argmin(test_scores)]\n",
    "best_model = KNeighborsRegressor(**best_final_model.get_params())\n",
    "best_model.fit(train_prep, y_train)\n",
    "y_test_pred = best_iteration_model.predict(test_prep)\n",
    "test_score = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8fdc62e3-7d0c-4fb2-8961-a54646722a1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['knn_results.joblib']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "\n",
    "dump({'model': best_model, 'scores': test_scores}, '../results/knn_results.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
